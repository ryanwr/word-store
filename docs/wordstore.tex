\documentclass[12pt]{article}

\usepackage{times}
%\usepackage[libertine,cmintegrals,cmbraces,vvarbb]{newtxmath}
%\usepackage[scaled=0.95]{inconsolata}
% Listings
\usepackage{listings}
\lstset{
  mathescape, 
  basicstyle=\itshape,
  xleftmargin=3em,
  literate={->}{$\rightarrow$}{2}
}
% Math tools
\usepackage{mathtools}
% Biblatex
\usepackage[
	backend=biber,
	style=numeric
]{biblatex}

\addbibresource{references.bib} %Imports bibliography file
% Tikz
\usepackage{tikz}
\usetikzlibrary{positioning}
% Caption and table caption spacing
\usepackage{caption}
\captionsetup[table]{skip=5pt}
\captionsetup[figure]{skip=14pt}
% Paragraph spacing
\usepackage{parskip}
% Geometry
\usepackage{geometry}
\geometry{
  top=0.5in,
  inner=1in,
  outer=1in,
  bottom=1in,
  headheight=3ex,
  headsep=2ex,
}
% Fancy header
\usepackage{fancyhdr}
% Placeins
\usepackage{placeins}
% Minted
\usepackage[outputdir=build]{minted}
\usemintedstyle{trac}
% Cenered Minted
\usepackage{xpatch,letltxmacro}
\LetLtxMacro{\cinputminted}{\inputminted}
\xpretocmd{\cinputminted}{\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}}{}{}
% Clickable links
\usepackage[hidelinks]{hyperref}
\hypersetup{
    colorlinks=false, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
}
% PGFplots
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=newest}
% Fix for PGFplots not handling nan in linear regression
\makeatletter
% #1: keys
\def\pgfplotstable@linear@regression#1{%
    \begingroup
    \pgfqkeys{/pgfplots/table/create col/linear regression}{/pgf/fpu,#1}%
    \pgfkeysgetvalue{/pgfplots/table/create col/linear regression/x}{\pgfplotstable@xsrc}%
    \pgfkeysgetvalue{/pgfplots/table/create col/linear regression/y}{\pgfplotstable@ysrc}%
    \pgfkeysgetvalue{/pgfplots/table/create col/linear regression/table}{\pgfplotstable@table}%
    \pgfkeysgetvalue{/pgfplots/table/create col/linear regression/xmode}{\pgfplotstable@xmode}%
    \pgfkeysgetvalue{/pgfplots/table/create col/linear regression/ymode}{\pgfplotstable@ymode}%
    \pgfkeysgetvalue{/pgfplots/table/create col/linear regression/variance}{\pgfplotstable@variance@colname}%
    \pgfkeysgetvalue{/pgfplots/table/create col/linear regression/variance list}{\pgfplotstable@variance@list}%
    \pgfkeysgetvalue{/pgfplots/table/create col/linear regression/variance src}{\pgfplotstable@variance@table}%
    %
    \ifx\pgfplotstable@table\pgfutil@empty
        \pgfutil@ifundefined{pgfplotstablename}{}{% query the name of the actual table struct
            \let\pgfplotstable@table=\pgfplotstablename
        }%
    \fi
    \ifx\pgfplotstable@table\pgfutil@empty
        \pgfplots@error{Sorry, I couldn't determine a value for create col/linear regression/table. Which table should I load?}%
    \fi
    \ifx\pgfplotstable@xsrc\pgfutil@empty
        \pgfplotsifinaddplottablestruct{%
            \pgfutil@ifundefined{pgfplots@plot@tbl@x}{}{%
                \let\pgfplotstable@xsrc=\pgfplots@plot@tbl@x
                \ifx\pgfplotstable@ysrc\pgfutil@empty
                    \pgfplotstablegetcolsof\pgfplots@table
                    \ifnum\pgfplotsretval=2
                    \else
                        \pgfplotsthrow{invalid argument}{\pgfplotstable@ysrc}{Sorry, I don't which column should be used as `y' for the linear regression. Please provide 'linear regression={y=<colname>}'}\pgfeov%
                    \fi
                \fi
            }%
        }{}%
    \fi
    \ifx\pgfplotstable@xsrc\pgfutil@empty
        \def\pgfplotstable@xsrc{[index]0}%
    \fi
    \ifx\pgfplotstable@ysrc\pgfutil@empty
        \def\pgfplotstable@ysrc{[index]1}%
    \fi
    %
    \t@pgfplots@toka=\expandafter{\pgfplotstable@table}%
    \t@pgfplots@tokb=\expandafter{\pgfplotstable@xsrc}%
    \t@pgfplots@tokc=\expandafter{\pgfplotstable@ysrc}%
    \edef\pgfplots@loc@TMPa{{\the\t@pgfplots@tokb}\noexpand\of{\the\t@pgfplots@toka}}%
    \edef\pgfplots@loc@TMPb{{\the\t@pgfplots@tokc}\noexpand\of{\the\t@pgfplots@toka}}%
    \expandafter\pgfplotstablegetcolumn\pgfplots@loc@TMPa\to\pgfplotstable@X
    \expandafter\pgfplotstablegetcolumn\pgfplots@loc@TMPb\to\pgfplotstable@Y
    %
    \edef\pgfplotstable@xmode{\pgfplotstable@xmode}%
    \expandafter\pgfplotstable@linear@regression@prepare@mode\expandafter{\pgfplotstable@xmode}{x}%%
    \edef\pgfplotstable@ymode{\pgfplotstable@ymode}%
    \expandafter\pgfplotstable@linear@regression@prepare@mode\expandafter{\pgfplotstable@ymode}{y}%%
    %
    \ifx\pgfplotstable@variance@list\pgfutil@empty
        % check 'variance' key (loaded from table)
        \pgfplotslistnewempty\pgfplotstable@VARIANCE
        \ifx\pgfplotstable@variance@colname\pgfutil@empty
        \else
            \ifx\pgfplotstable@variance@table\pgfutil@empty
                \t@pgfplots@toka=\expandafter{\pgfplotstable@table}%
                \t@pgfplots@tokb=\expandafter{\pgfplotstable@variance@colname}%
                \edef\pgfplots@loc@TMPa{{\the\t@pgfplots@tokb}\noexpand\of{\the\t@pgfplots@toka}}%
                \expandafter\pgfplotstablegetcolumn\pgfplots@loc@TMPa\to\pgfplotstable@VARIANCE
            \else
                \t@pgfplots@toka=\expandafter{\pgfplotstable@variance@colname}%
                \t@pgfplots@tokb=\expandafter{\pgfplotstable@variance@table}%
                \edef\pgfplotstable@loc@TMPa{%
                    \noexpand\pgfplotstablegetcolumn{\the\t@pgfplots@toka}\noexpand\of{\the\t@pgfplots@tokb}\noexpand\to\noexpand\pgfplotstable@VARIANCE}%
                \pgfplotstable@loc@TMPa
            \fi
        \fi
    \else
        % load from list:
        \expandafter\pgfplotslistnew\expandafter\pgfplotstable@VARIANCE\expandafter{\pgfplotstable@variance@list}%
    \fi
    %
    \pgfplotslistnewempty\pgfplotstable@Xparsed
    %
    \pgfmathfloatcreate{0}{0.0}{0}%
    \let\pgfplotstable@S=\pgfmathresult
    \let\pgfplotstable@Sxx=\pgfmathresult
    \let\pgfplotstable@Sx=\pgfmathresult
    \let\pgfplotstable@Sy=\pgfmathresult
    \let\pgfplotstable@Sxy=\pgfmathresult
    \pgfutil@loop
    \pgfplotslistcheckempty\pgfplotstable@X
    \ifpgfplotslistempty
        \pgfplots@loop@CONTINUEfalse
    \else
        \pgfplots@loop@CONTINUEtrue
    \fi
    \ifpgfplots@loop@CONTINUE
        \pgfplotslistpopfront\pgfplotstable@X\to\pgfplotstable@x
        \pgfplotslistpopfront\pgfplotstable@Y\to\pgfplotstable@y
        %
        \pgfplotstableparsex{\pgfplotstable@x}%
        \let\pgfplotstable@x=\pgfmathresult
        \expandafter\pgfplotslistpushback\pgfmathresult\to\pgfplotstable@Xparsed
        \pgfplotstableparsey{\pgfplotstable@y}%
        \let\pgfplotstable@y=\pgfmathresult
        \pgfmathfloatifflags{\pgfplotstable@y}{3}{}{% <---- This is new. The "3" stands for "nan"
        %
        \pgfplotslistcheckempty\pgfplotstable@VARIANCE
        \ifpgfplotslistempty
            \pgfmathfloatcreate{1}{1.0}{0}%
            \let\pgfplotstable@invsqr=\pgfmathresult
        \else
            \pgfplotslistpopfront\pgfplotstable@VARIANCE\to\pgfplotstable@variance
            \pgfmathfloatparsenumber{\pgfplotstable@variance}%
            \let\pgfplotstable@variance=\pgfmathresult
            \pgfmathfloatmultiply@{\pgfplotstable@variance}{\pgfplotstable@variance}%
            \let\pgfplotstable@sqr=\pgfmathresult
            \pgfmathfloatreciprocal@{\pgfplotstable@sqr}%
            \let\pgfplotstable@invsqr=\pgfmathresult
        \fi
        %
        \pgfmathfloatadd@{\pgfplotstable@S}{\pgfplotstable@invsqr}%
        \let\pgfplotstable@S=\pgfmathresult
        %
        \pgfmathfloatmultiply@{\pgfplotstable@x}{\pgfplotstable@invsqr}%
        \let\pgfplots@table@accum=\pgfmathresult
        \pgfmathfloatadd@{\pgfplotstable@Sx}{\pgfplots@table@accum}%
        \let\pgfplotstable@Sx=\pgfmathresult
        %
        \pgfmathfloatmultiply@{\pgfplotstable@x}{\pgfplots@table@accum}%
        \let\pgfplots@table@accum=\pgfmathresult
        \pgfmathfloatadd@{\pgfplotstable@Sxx}{\pgfplots@table@accum}%
        \let\pgfplotstable@Sxx=\pgfmathresult
        %
        \pgfmathfloatmultiply@{\pgfplotstable@y}{\pgfplotstable@invsqr}%
        \let\pgfplots@table@accum=\pgfmathresult
        \pgfmathfloatadd@{\pgfplotstable@Sy}{\pgfplots@table@accum}%
        \let\pgfplotstable@Sy=\pgfmathresult
        %
        \pgfmathfloatmultiply@{\pgfplotstable@x}{\pgfplots@table@accum}%
        \let\pgfplots@table@accum=\pgfmathresult
        \pgfmathfloatadd@{\pgfplotstable@Sxy}{\pgfplots@table@accum}%
        \let\pgfplotstable@Sxy=\pgfmathresult
        }% <---- This is new.
    \pgfutil@repeat
    %
    \pgfmathparse{\pgfplotstable@S * \pgfplotstable@Sxx - \pgfplotstable@Sx *\pgfplotstable@Sx}%
    \let\pgfplotstable@delta=\pgfmathresult
    %
    \pgfmathparse{(\pgfplotstable@S * \pgfplotstable@Sxy - \pgfplotstable@Sx * \pgfplotstable@Sy) / \pgfplotstable@delta}%
    \let\pgfplotstable@a=\pgfmathresult
    %
    \pgfmathparse{(\pgfplotstable@Sxx * \pgfplotstable@Sy - \pgfplotstable@Sx * \pgfplotstable@Sxy) / \pgfplotstable@delta}%
    \let\pgfplotstable@b=\pgfmathresult
    %
    \pgfplotslistnewempty\pgfplotstable@RESULT
    \pgfplotslistforeachungrouped\pgfplotstable@Xparsed\as\pgfplotstable@x{%
        \pgfmathfloatmultiply@{\pgfplotstable@x}{\pgfplotstable@a}%
        \let\pgfplotstable@tmp=\pgfmathresult
        \pgfmathfloatadd@{\pgfplotstable@tmp}{\pgfplotstable@b}%
        \ifx\pgfplotstableparseylogbase\pgfutil@empty
        \else
            \pgfplotstableparseyinv@{\pgfmathresult}%
        \fi
        \pgfmathfloattosci{\pgfmathresult}%
        \expandafter\pgfplotslistpushback\pgfmathresult\to\pgfplotstable@RESULT
    }%
    \pgfmathfloattosci\pgfplotstable@a
    \let\pgfplotstable@a=\pgfmathresult
    %
    \pgfmathfloattosci\pgfplotstable@b
    \let\pgfplotstable@b=\pgfmathresult
    %
    \global\let\pgfplotstableregressiona\pgfplotstable@a%
    \global\let\pgfplotstableregressionb\pgfplotstable@b%
    \let\pgfplotsretval=\pgfplotstable@RESULT
    \pgfmath@smuggleone\pgfplotsretval
    \endgroup
}%
\makeatother
\pgfplotstableread[col sep = comma]{addwords.csv}\addwords
\pgfplotstableread[col sep = comma]{countwords.csv}\countwords
\pgfplotstableread[col sep = comma]{removewords.csv}\removewords
% Title section
\usepackage{titlesec}% http://ctan.org/pkg/titlesec
\titleformat{\section}%
  [hang]% <shape>
  {\normalfont\bfseries\Large}% <format>
  {}% <label>
  {0pt}% <sep>
  {}% <before code>
  
%\renewcommand\thesubsection{\Alph{subsection}} 
%\renewcommand{\thesubsubsection}{\thesubsection.\Roman{subsubsection}}
\renewcommand\thesubsection{} 
\renewcommand{\thesubsubsection}{}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{Ryan Welch - 150278980}
\renewcommand{\headrulewidth}{0pt}

\title{Algorithms and Data Structures in an Object Oriented Framework - Coursework}
\author{\textit{Ryan Welch}}
\date{\today}

\begin{document}

\maketitle
\thispagestyle{fancy}

\tableofcontents

\clearpage

\section{Introduction}
The task was to create an abstract dictionary data structure defined by an interface and implemented by an efficient data structure. The requirements are to implement a dictionary that can insert, remove and count the occurrences or words. I started with a reference 'array and count' implementation where the words are stored in a virtual array which doubles in size every time it runs out of space in the real array. This is a simple way to check more complicated implementations for correctness as there is very little complexity in this reference implementation.

\begin{figure}[!htp]
\centering
\cinputminted[mathescape,breaklines=true,fontsize=\footnotesize,baselinestretch=0.8,firstline=3,lastline=9]{java}{../src/main/java/com/ryanwelch/wordstore/WordStore.java}
\caption{WordStore interface}
\label{lst:wordstore_interface}
\end{figure}

I chose to use a hash table as it provides in the average case $O(1)$ lookup, insertion and removal. Whereas the reference implementation is an abysmal $O(n)$ for insertion, removal and lookup. While in theory a hash table is actually $O(n)$, in the rare case where all elements have the same hash, in practice it performs closer to $O(1)$. Another data structure I considered was a binary tree, in particular a self-balancing binary tree such as a red-black tree which has a guaranteed $O(\log{}n)$ worst case however the hash table appears to be a better, more performant choice \cite{introtoalgorithms}. One disadvantage with a hash table is it wastes some memory as the array will never be full, however we are not very concerned with it's memory use as long as it is not excessive.

\section{Structure}
A hash table is simply an array, however instead of placing items linearly we insert them at a position determined by a hash function. The hash function aims for a uniform distribution of the items across all buckets. In the case where the hash function chooses the same bucket for two different elements we must store both elements in the same bucket. There are two design choices for dealing with collisions: open addressing and separate chaining. Separate chaining is where each bucket contains a list structure to store all elements at the same position, however requiring the additional step of iterating the list to find the element, usually $O(n)$. Open addressing is where if the bucket is already occupied you traverse the array until you find the next unoccupied slot, this is also $O(n)$ however incurs a hidden penalty in which you are occupying another position in the table and increasing the chances of another collision more rapidly.

I chose to use separate chaining with a linked list data structure for storing collisions, this keeps the complexity low. It would be more efficient in most cases to use a better performing list structure such as self-balancing binary trees to store the collisions however it increases a lot of additional code and complexity for a small increase in performance, it also may reduce performance on smaller hash table due to the increased insertion cost of a balancing binary tree. It would reduce the worst case in theory from $O(n)$ to $O(\log{}n)$ however I found there were not many collisions and hence only a small cost in traversing the list.

\subsection{Insertion}
Insertion is a worst-case $O(n)$ operation however it is in most cases $O(1)$, the code works by generating the hash for the word and mapping it to a position in the array, and then inserting the word into that position. Each position contains a linked list to deal with possible collisions of a different word having the same hash, if the list already contains the same word the count of the node which is our linked list cell is increased to account for the same word being added again. If the word is not found it is added to the end of the linked list.

\begin{figure}[!htp]
\centering
\cinputminted[mathescape,breaklines=true,fontsize=\footnotesize,baselinestretch=0.8,firstline=106,lastline=128]{java}{../src/main/java/com/ryanwelch/wordstore/WordStoreHashTableImp.java}
\caption{Code fragment from the add method}
\label{lst:addition_method}
\end{figure}

In the implementation, the WordNode class is a the cell that represents the linked lists in the hash table, each WordNode can hold a word, the count of how many times it has been added and an optional pointer to another WordNode. The bucket array is simply an array of pointers to the root WordNodes which form the linked lists for each bucket.

\subsection{Deletion}
Removing a word works very similar to insertion but in reverse. First we find the bucket for the word by calculating the index of the bucket it is stored at using its hash. If the bucket is not empty then the linked list at the bucket is traversed until it finds the node for the word. If the node has a count greater than one we remove the word by simply decreasing the count which represents the number of times it has been added to the word store. If however the count is 1 we remove the node completely by altering the pointers in the linked list, the parent's next node is set to the next node of the one we are removing (it may be null). The deletion method has the same time complexity as the addition, with $O(1)$ on average and $O(n)$ at worst.

\begin{figure}[!htp]
\centering
\cinputminted[mathescape,breaklines=true,fontsize=\footnotesize,baselinestretch=0.8,firstline=152,lastline=177]{java}{../src/main/java/com/ryanwelch/wordstore/WordStoreHashTableImp.java}
\caption{Code fragment from the remove method}
\label{lst:remove_method}
\end{figure}

\subsection{Retrieval}
Retrieval from the hash table again has the same big-O categories with $O(1)$ on average and $O(n)$ at worst case. It works again very similarly, it calculates the hash to access the correct bucket. If the bucket contains the word in the linked list, it returns the value of the count stored in that node as it may represent more than one copy of the same word. If it is not found in the list or the bucket is empty the count is 0.

\begin{figure}[!htp]
\centering
\cinputminted[mathescape,breaklines=true,fontsize=\footnotesize,baselinestretch=0.8,firstline=133,lastline=145]{java}{../src/main/java/com/ryanwelch/wordstore/WordStoreHashTableImp.java}
\caption{Code fragment from the count method}
\label{lst:count_method}
\end{figure}

\section{Collisions}
\subsection{Resizing}
An additional step is performed before addition or removal, a call is made to the resize method. A resize is needed to prevent the number of collisions from becoming too high. For example, if the array of linked lists is created initially with a size of 100 and 1 million items are added there will be almost 1 million collisions as there is not enough space to store the 1 million items.

In order to prevent this we resize the array when the array is almost full, we do this by calculating a load factor which is the number of items (WordNode's) in the hash table over the number of buckets (our array size). Then if the load factor is over an arbitrary value, such as 0.7 we double the array size and move all the items to their new hash positions. The arbitrary value of 0.7 worked well for me, it equates to when the array is 70\%, the closer the load factor gets to 1 the more likely you are to collide as in a perfect distribution it is when there are no more spaces in the array, 0.7 provides a nice buffer as the hash function can not be perfect.

\begin{figure}[!htp]
\centering
\inputminted[mathescape,breaklines=true,fontsize=\footnotesize,baselinestretch=0.8,firstline=39,lastline=45]{java}{../src/main/java/com/ryanwelch/wordstore/WordStoreHashTableImp.java}
\caption{Code fragment calculating load factor and deciding whether to resize}
\label{lst:load_factor_code}
\end{figure}

Whenever we resize the bucket array it becomes necessary to recompute the position for all the items as the position depends on the size of the array as we map the hash (which is the always the same) to the buckets. This means we take a large performance hit on insertion or removal whenever we need to resize, however despite this it provides better performance for future insertion and removal due to the reduced number of collisions. This means that when we resize, our big-O is $O(n)$ which is the same as the formal big-O of the hash table.

In order to avoid resizing if you know the size of the data in advance, the WordStore provides a constructor which accepts the number of items that will be added. This can be used as the initial value for the bucket array which means we are guaranteed to not need to resize when adding the given number of elements. Additionally, we round the array size up to the next power of two and double it on resize to ensure it is still a power of two, the array size is required to be a power of two due to our hash function which is explained later.

\subsection{The Hash Function}
The hash function is the most important part of the hash table, it should distribute the items uniformly across all the buckets in order to avoid collisions, our items in this case are strings so we must find a way to represent a string as a 32bit integer. It is not possible to have a unique integer for every string as the number of possible strings is infinitely larger than a 32bit integer. However we can try to randomize the distribution of 32bit integers so that the probability of another string having the same hash is $\frac{1}{2^{32}}$.

\begin{figure}[!htp]
\centering
\cinputminted[mathescape,breaklines=true,fontsize=\footnotesize,baselinestretch=0.8,firstline=84,lastline=100]{java}{../src/main/java/com/ryanwelch/wordstore/WordStoreHashTableImp.java}
\caption{The hash function and mapping to bucket array}
\label{lst:hash_function}
\end{figure}

The first attempt at a hash function was simply adding the ascii value of each character multiplied by a prime to ensure that the same letters but in different orders resulted in different hashes. This worked okay however later tried more algorithms for generating a hash, two of which where djb2 \cite{hashfunctionsdjb2} and FNV \cite{hashfunctionsfnv}, both offer an improvement however I chose djb2 as it resulted in slightly less collisions on my test data. It operates on bytes of data rather than characters and follows a similar pattern by multiplying by a number before adding the next byte. However it uses bit shifts to achieve the multiplication as it is much faster. Computing the hash is often the slowest part of a hash table and hence we aim to make it as fast as possible but still providing a good enough result to avoid collisions.

Once we have a 32bit integer as a hash for our word, we need to map the integer to a bucket in our hash table. Taking the modulus of the integer would work however it is relatively slow so instead we use a bitwise AND operation on the lower bits of the integer, this can mean we do not always use the full information available in the hash but it is faster than using modulus and in this case we care about performance over a perfect hash. A requirement of doing a bitwise AND instead of modulus is making sure that table size is a power of two. This works as to map the hash to the bucket we take the n rightmost number of bits, each additional bit increases our output size by double, so if the table size is $2^n$ in size, we use the n most significant bits.

\section{Comparison}

In order to show the hash table's performance and verify its big-O category I compared it against a simple Array and count data structure and a Binary Tree data structure. The Array implementation was also used as a comparison whilst developing the hash table version. In the graphs below I tested each data structure with a fixed number of insertions, deletions and retrievals whilst changing the size of previously stored data. This should show the big-O category as its category defines how long an individual operation will take depending on the how much data is already in the structure. I measured the time taken with as much precision as possible using javas \texttt{System.nanoTime()}.

The first test is shown in Figure \ref{fig:add_chart}, it shows the trend of all three data structures roughly constant, however noticibly the Binary Tree has a larger gradient than the other two. This is because the binary tree has a big-O of $O(\log(n))$ whilst the insertion for the Array and insertion for the Hash Table are $O(1)$. The Hash Table takes a constant amount of time longer than the Array to insert which is most likely due to the higher costs in insertion from calculating the hash of each word.


\begin{figure}[!htb]
\begin{tikzpicture}
    \begin{axis}[
    		ymin=0,
    		xmode=log,
            width=0.9\textwidth,
            height=0.4\textwidth,
            legend style={
            	draw=none,
            	fill=none, 
            	at={(0.5,-0.25)},
                anchor=north,
                legend columns=-1,
                /tikz/every even column/.append style={column sep=0.5cm}
            },
            ytick style={draw=none},
            nodes near coords align={vertical},
            axis lines*=left,
            yticklabel style={/pgf/number format/fixed},
			scaled y ticks=false,
			xticklabel style={/pgf/number format/fixed},
			scaled x ticks=false,
            ylabel={Time to add 10000 words in nanoseconds (ns)},
            xlabel={Number of words in word store},
            axis on top
        ]
        \addplot[mark=none,color=orange] table[x=size,y={create col/linear regression={y=ARRAY}}] {\addwords};
        \addplot[mark=none,color=red] table[x=size,y={create col/linear regression={y=BINARY_TREE}}] {\addwords};
        \addplot[mark=none,color=blue] table[x=size,y={create col/linear regression={y=HASH_TABLE}}] {\addwords};
        \addplot[only marks, mark=x, mark options={solid}, color=orange, forget plot] table[x=size,y=ARRAY]{\addwords};
        \addplot[only marks, mark=x, mark options={solid}, color=red, forget plot] table[x=size,y=BINARY_TREE]{\addwords};
        \addplot[only marks, mark=x, mark options={solid}, color=blue, forget plot] table[x=size,y=HASH_TABLE]{\addwords};
        \legend{Array, Binary Tree, Hash Table}
    \end{axis}
\end{tikzpicture}
\caption[The LOF caption]{Graph of adding n words to word store\footnotemark[1]}
\label{fig:add_chart}
\end{figure}


The count test data is shown in Figure \ref{fig:count_chart}, the Array data is not included in the graph as it is much slower however it is included in Table \ref{tab:count_data} in the appendix for completeness. The data shows a similar trend as in the addition test, the Hash Table remains constant at around 2ms but the Binary Tree increases linearly in $O(\log(n))$ time. The Array performs at a very slow $O(n)$ and struggles at anything larger than an initial size of 1 million as it must check every position in the array in contrast to the Hash Table which simply looks up the position for the node.


\begin{figure}[!htb]
\begin{tikzpicture}
    \begin{axis}[
    		ymin=0,
    		ymax=10000000,
    		xmode=log,
            width=0.9\textwidth,
            height=0.4\textwidth,
            legend style={
            	draw=none,
            	fill=none, 
            	at={(0.5,-0.25)},
                anchor=north,
                legend columns=-1,
                /tikz/every even column/.append style={column sep=0.5cm}
            },
            ytick style={draw=none},
            nodes near coords align={vertical},
            axis lines*=left,
            yticklabel style={/pgf/number format/fixed},
			scaled y ticks=false,
			xticklabel style={/pgf/number format/fixed},
			scaled x ticks=false,
            ylabel={Time to add 10000 words in nanoseconds (ns)},
            xlabel={Number of words in word store},
            axis on top
        ]
        %\addplot[mark=none,color=orange] table[x=size,y={create col/linear regression={y=ARRAY}}] {\countwords};
        \addplot[mark=none,color=red] table[x=size,y={create col/linear regression={y=BINARY_TREE}}] {\countwords};
        \addplot[mark=none,color=blue] table[x=size,y={create col/linear regression={y=HASH_TABLE}}] {\countwords};
        %\addplot[only marks, mark=x, mark options={solid}, color=orange, forget plot] table[x=size,y=ARRAY]{\countwords};
        \addplot[only marks, mark=x, mark options={solid}, color=red, forget plot] table[x=size,y=BINARY_TREE]{\countwords};
        \addplot[only marks, mark=x, mark options={solid}, color=blue, forget plot] table[x=size,y=HASH_TABLE]{\countwords};
        %\legend{Array, Binary Tree, Hash Table}
        \legend{Binary Tree, Hash Table}
    \end{axis}
\end{tikzpicture}
\caption[The LOF caption]{Graph of counting n words in word store\footnotemark[1]}
\label{fig:count_chart}
\end{figure}

Finally, the remove method test in Figure \ref{fig:remove_chart} shows again the same patterns, the Hash Table performs at $O(1)$ and the Binary Tree at $O(\log(n))$. This shows just how efficient the hash table is, and is capable to deal with vast amount of data, more than I have tested. The hash table is more likely to be bounded by memory than processing time on larger data sets. It does however also incur a a large penalty when resizing to accommodate more elements which is not represented accurately in this data, however at a large scale this would be an issue with the hash table which could be mitigated by, after a resize, maintaining more than one active hash function and on every insert or delete operation a portion of data is relocated to the new array \cite{resizinghashtables}.
Hash Tables are a large improvement over a simple 'array and count' structure, there are still improvements that can be made to my implementation however it would also increase the complexity and my implementation still performs as expected.

\begin{figure}[!htb]
\begin{tikzpicture}
    \begin{axis}[
    		ymin=0,
    		ymax=10000000,
    		xmode=log,
            width=0.9\textwidth,
            height=0.4\textwidth,
            legend style={
            	draw=none,
            	fill=none, 
            	at={(0.5,-0.25)},
                anchor=north,
                legend columns=-1,
                /tikz/every even column/.append style={column sep=0.5cm}
            },
            ytick style={draw=none},
            nodes near coords align={vertical},
            axis lines*=left,
            yticklabel style={/pgf/number format/fixed},
			scaled y ticks=false,
			xticklabel style={/pgf/number format/fixed},
			scaled x ticks=false,
            ylabel={Time to add 10000 words in nanoseconds (ns)},
            xlabel={Number of words in word store},
            axis on top
        ]
        %\addplot[mark=none,color=orange] table[x=size,y={create col/linear regression={y=ARRAY}}] {\removewords};
        \addplot[mark=none,color=red] table[x=size,y={create col/linear regression={y=BINARY_TREE}}] {\removewords};
        \addplot[mark=none,color=blue] table[x=size,y={create col/linear regression={y=HASH_TABLE}}] {\removewords};
        %\addplot[only marks, mark=x, mark options={solid}, color=orange, forget plot] table[x=size,y=ARRAY]{\removewords};
        \addplot[only marks, mark=x, mark options={solid}, color=red, forget plot] table[x=size,y=BINARY_TREE]{\removewords};
        \addplot[only marks, mark=x, mark options={solid}, color=blue, forget plot] table[x=size,y=HASH_TABLE]{\removewords};
        %\legend{Array, Binary Tree, Hash Table}
        \legend{Binary Tree, Hash Table}
    \end{axis}
\end{tikzpicture}
\caption[The LOF caption]{Graph of removing n words from word store\footnotemark}
\label{fig:remove_chart}
\end{figure}
\footnotetext{Tests performed on an Intel Core i7-3770k @ 3.5Ghz}



\FloatBarrier % Prevent figures floating into section

\clearpage

\printbibliography[heading=bibnumbered]

\section{Appendix}
\subsection{Timing data}
\begin{table}[!htb]
\centering
\caption{Add n words to word store} 
\label{tab:add_data} 
\pgfplotstabletypeset[
    string type,
    string replace*={nan}{N/A},
    columns/size/.style={column name=Number of words, column type={|l}},
    columns/ARRAY/.style={column name=Array (ns), column type={|l}},
    columns/BINARY_TREE/.style={column name=Binary Tree (ns), column type={|l}},
    columns/HASH_TABLE/.style={column name=Hash Table (ns), column type={|l|}},
    every head row/.style={before row=\hline,after row=\hline},
    every last row/.style={after row=\hline},
    ]{\addwords}
\end{table}

\begin{table}[!htb]
\centering
\caption{Count n words in word store} 
\label{tab:count_data} 
\pgfplotstabletypeset[
    string type,
    string replace*={nan}{N/A},
    columns/size/.style={column name=Number of words, column type={|l}},
    columns/ARRAY/.style={column name=Array (ns), column type={|l}},
    columns/BINARY_TREE/.style={column name=Binary Tree (ns), column type={|l}},
    columns/HASH_TABLE/.style={column name=Hash Table (ns), column type={|l|}},
    every head row/.style={before row=\hline,after row=\hline},
    every last row/.style={after row=\hline},
    ]{\countwords}
\end{table}

\begin{table}[!htb]
\centering
\caption{Remove n words from word store} 
\label{tab:remove_data} 
\pgfplotstabletypeset[
    string type,
    string replace*={nan}{N/A},
    columns/size/.style={column name=Number of words, column type={|l}},
    columns/ARRAY/.style={column name=Array (ns), column type={|l}},
    columns/BINARY_TREE/.style={column name=Binary Tree (ns), column type={|l}},
    columns/HASH_TABLE/.style={column name=Hash Table (ns), column type={|l|}},
    every head row/.style={before row=\hline,after row=\hline},
    every last row/.style={after row=\hline},
    ]{\removewords}
\end{table}

\FloatBarrier

\subsection{WordStoreHashTableImp}
\inputminted[mathescape,breaklines=true,fontsize=\footnotesize,baselinestretch=0.75,linenos=true]{java}{../src/main/java/com/ryanwelch/wordstore/WordStoreHashTableImp.java}

\clearpage

\subsection{WordStoreArrayImp}
\inputminted[mathescape,breaklines=true,fontsize=\footnotesize,baselinestretch=0.75,linenos=true]{java}{../src/main/java/com/ryanwelch/wordstore/WordStoreArrayImp.java}

\end{document}
